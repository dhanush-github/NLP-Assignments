{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JGKG8DaDAK_O"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c2YtR1hqAYPg",
    "outputId": "c3fb0fc9-1498-4241-a026-6a6f66a065dd"
   },
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xk8zmdbwjasR"
   },
   "source": [
    "Read the text from file and print the first few lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OCtxifk8Act1",
    "outputId": "e16a910a-7e21-47b3-9ac3-097f81d87f9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n"
     ]
    }
   ],
   "source": [
    "with open(path_to_file) as f:\n",
    "  lines = f.read().splitlines()\n",
    "for line in lines[:20]:\n",
    "  print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "bnJDAbPCAeqn"
   },
   "outputs": [],
   "source": [
    "text_ds = tf.data.TextLineDataset(path_to_file).filter(lambda x: tf.cast(tf.strings.length(x), bool))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "beik8MDXj0aM"
   },
   "source": [
    "**Vectorize sentences from corpus**\n",
    "\n",
    "we can use text vectorization layer to vectorize sentences from the corpus, first few sentences above the text needs to be in one case and punctuation needs to be removed. to do this define a custom_standardisation function that can be used in text vectorization layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "3P0nCidXAnte"
   },
   "outputs": [],
   "source": [
    "# Now, create a custom standardization function to lowercase the text and\n",
    "# remove punctuation.\n",
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  return tf.strings.regex_replace(lowercase,\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')\n",
    "\n",
    "\n",
    "# Define the vocabulary size and the number of words in a sequence.\n",
    "vocab_size = 4096\n",
    "sequence_length = 10\n",
    "\n",
    "# Use the `TextVectorization` layer to normalize, split, and map strings to\n",
    "# integers. Set the `output_sequence_length` length to pad all samples to the\n",
    "# same length.\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "76Vmp-p2BaEG"
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "qh-9v9XfDJaF"
   },
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "35HQURuhAs26"
   },
   "outputs": [],
   "source": [
    "vectorize_layer.adapt(text_ds.batch(1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i7xkMnaFBPX9",
    "outputId": "4d99699b-6b47-443b-ba02-222103c7303c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'the', 'and', 'to', 'i', 'of', 'you', 'my', 'a', 'that', 'in', 'is', 'not', 'for', 'with', 'me', 'it', 'be', 'your']\n"
     ]
    }
   ],
   "source": [
    "# Save the created vocabulary for reference.\n",
    "inverse_vocab = vectorize_layer.get_vocabulary()\n",
    "print(inverse_vocab[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6VHNZ8IkDPY6"
   },
   "outputs": [],
   "source": [
    "# Vectorize the data in text_ds.\n",
    "text_vector_ds = text_ds.batch(1024).prefetch(tf.data.AUTOTUNE).map(vectorize_layer).unbatch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7fZqGiQlTLi"
   },
   "source": [
    "**Obtain Sequences from the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LJToItcwDXuS",
    "outputId": "bf94aa35-f83e-4bad-f405-0dbf8fbf1ea6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32777\n"
     ]
    }
   ],
   "source": [
    "sequences = list(text_vector_ds.as_numpy_iterator())\n",
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3EQR9sC8luXZ"
   },
   "source": [
    "**Inspect few examples from the sequence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QdqbAm1NDow-",
    "outputId": "2ad8bd62-893b-4b0f-c5bf-7baf7a38be84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 89 270   0   0   0   0   0   0   0   0] => ['first', 'citizen', '', '', '', '', '', '', '', '']\n",
      "[138  36 982 144 673 125  16 106   0   0] => ['before', 'we', 'proceed', 'any', 'further', 'hear', 'me', 'speak', '', '']\n",
      "[34  0  0  0  0  0  0  0  0  0] => ['all', '', '', '', '', '', '', '', '', '']\n",
      "[106 106   0   0   0   0   0   0   0   0] => ['speak', 'speak', '', '', '', '', '', '', '', '']\n",
      "[ 89 270   0   0   0   0   0   0   0   0] => ['first', 'citizen', '', '', '', '', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences[:5]:\n",
    "  print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "-KTHFEcHEaVK"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "-c3EbdykFduf"
   },
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "HSvjqqIuGVgJ"
   },
   "outputs": [],
   "source": [
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lQV8Ze1el-kq"
   },
   "source": [
    "# **PREPARATION OF POSITIVE AND NEGATIVE SAMPLES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "rG_8dNzCEM5C"
   },
   "outputs": [],
   "source": [
    "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
    "# (int-encoded sentences) based on window size, number of negative samples\n",
    "# and vocabulary size.\n",
    "def generate_training_data(sequences, window_size, num_ns, vocab_size):\n",
    "  # Elements of each training example are appended to these lists.\n",
    "  targets, contexts, labels = [], [], []\n",
    "\n",
    "  # Build the sampling table for `vocab_size` tokens.\n",
    "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "  # Iterate over all sequences (sentences) in the dataset.\n",
    "  for sequence in tqdm.tqdm(sequences):\n",
    "\n",
    "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
    "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "          sequence,\n",
    "          vocabulary_size=vocab_size,\n",
    "          sampling_table=sampling_table,\n",
    "          window_size=window_size,\n",
    "          negative_samples=0)\n",
    "\n",
    "    # Iterate over each positive skip-gram pair to produce training examples\n",
    "    # with a positive context word and negative samples.\n",
    "    for target_word, context_word in positive_skip_grams:\n",
    "      context_class = tf.expand_dims(\n",
    "          tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "          true_classes=context_class,\n",
    "          num_true=1,\n",
    "          num_sampled=num_ns,\n",
    "          unique=True,\n",
    "          range_max=vocab_size,\n",
    "          #seed=SEED,\n",
    "          name=\"negative_sampling\")\n",
    "\n",
    "      # Build context and label vectors (for one target word)\n",
    "      negative_sampling_candidates = tf.expand_dims(\n",
    "          negative_sampling_candidates, 1)\n",
    "\n",
    "      context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
    "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "      # Append each element from the training example to global lists.\n",
    "      targets.append(target_word)\n",
    "      contexts.append(context)\n",
    "      labels.append(label)\n",
    "\n",
    "  return targets, contexts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vNxhbD4NIioZ",
    "outputId": "6d2bfa22-7d48-4710-ccba-1fcd97a5a636"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 32777/32777 [00:22<00:00, 1474.65it/s]\n"
     ]
    }
   ],
   "source": [
    "targets, contexts, labels = generate_training_data(\n",
    "    sequences=sequences,\n",
    "    window_size=2,\n",
    "    num_ns=4,\n",
    "    vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z_UaVc07GT9m",
    "outputId": "f628116a-e841-4c18-f380-48bb98358236"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "targets.shape: (65208,)\n",
      "contexts.shape: (65208, 5)\n",
      "labels.shape: (65208, 5)\n"
     ]
    }
   ],
   "source": [
    "targets = np.array(targets)\n",
    "contexts = np.array(contexts)[:,:,0]\n",
    "labels = np.array(labels)\n",
    "\n",
    "print('\\n')\n",
    "print(f\"targets.shape: {targets.shape}\")\n",
    "print(f\"contexts.shape: {contexts.shape}\")\n",
    "print(f\"labels.shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmd2HaqQmNi-"
   },
   "source": [
    "**Configure the dataset for performance**\n",
    "\n",
    "to perform efficient batching for potentially large number of training examples, we use tf.dataset.Data API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FI9VWMqbDzIT",
    "outputId": "03b20f33-29e4-463b-cecb-6461ca2549a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: (((1024,), (1024, 5)), (1024, 5)), types: ((tf.int64, tf.int64), tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1024\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DwfrWK8LI6Ok",
    "outputId": "962a77a1-7f9a-484b-d49b-0786f9c4a451"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset shapes: (((1024,), (1024, 5)), (1024, 5)), types: ((tf.int64, tf.int64), tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "HqFqQMGII806"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "-HklUZcRKoLa"
   },
   "outputs": [],
   "source": [
    "num_ns = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gzMVY2zEm8eW"
   },
   "source": [
    "# **TRAINING THE ALGORITHM**\n",
    "\n",
    "The word2vec model can be can be implemented as a classifier to distinguish between true context of the words obtained through negative samlping.\n",
    "we can perform a dot product multiplication between the embeddings of target and context words to obtain predictions for labels and compute the loss function against the true labels in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "TThwJirxJafr"
   },
   "outputs": [],
   "source": [
    "class Word2Vec(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim):\n",
    "    super(Word2Vec, self).__init__()\n",
    "    self.target_embedding = layers.Embedding(vocab_size,\n",
    "                                      embedding_dim,\n",
    "                                      input_length=1,\n",
    "                                      name=\"w2v_embedding\")\n",
    "    self.context_embedding = layers.Embedding(vocab_size,\n",
    "                                       embedding_dim,\n",
    "                                       input_length=num_ns+1)\n",
    "\n",
    "  def call(self, pair):\n",
    "    target, context = pair\n",
    "    # target: (batch, dummy?)  # The dummy axis doesn't exist in TF2.7+\n",
    "    # context: (batch, context)\n",
    "    if len(target.shape) == 2:\n",
    "      target = tf.squeeze(target, axis=1)\n",
    "    # target: (batch,)\n",
    "    word_emb = self.target_embedding(target)\n",
    "    # word_emb: (batch, embed)\n",
    "    context_emb = self.context_embedding(context)\n",
    "    # context_emb: (batch, context, embed)\n",
    "    dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
    "    # dots: (batch, context)\n",
    "    return dots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJ8FncM7o6lF"
   },
   "source": [
    "Define loss function and compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "RLgJPQBLJe7C"
   },
   "outputs": [],
   "source": [
    "def custom_loss(x_logit, y_true):\n",
    "      return tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jWWFxPZxpLYm"
   },
   "source": [
    "Initialising word2vec class with an embedding dimension of 100, and compiling the model using Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "PkFvyoF8Jm5W"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
    "word2vec.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7x6e2A8pkei"
   },
   "source": [
    "Defining the callback to log training statistics for Tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "A_26HruTJpzS"
   },
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jE1Io0GXpvw2"
   },
   "source": [
    "Train the model on dataset for some epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TbITKS2HK3zj",
    "outputId": "48f392f0-4cf1-417a-9e1f-6b735f8749d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "63/63 [==============================] - 6s 37ms/step - loss: 1.6084 - accuracy: 0.2299 5s\n",
      "Epoch 2/20\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 1.5923 - accuracy: 0.5213 1s -\n",
      "Epoch 3/20\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 1.5518 - accuracy: 0.5623 0s - loss: 1.5522 - accuracy: 0.56\n",
      "Epoch 4/20\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 1.4817 - accuracy: 0.5359\n",
      "Epoch 5/20\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 1.3974 - accuracy: 0.5430\n",
      "Epoch 6/20\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 1.3120 - accuracy: 0.5713\n",
      "Epoch 7/20\n",
      "63/63 [==============================] - 1s 21ms/step - loss: 1.2306 - accuracy: 0.6034\n",
      "Epoch 8/20\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 1.1544 - accuracy: 0.6355\n",
      "Epoch 9/20\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 1.0832 - accuracy: 0.6689\n",
      "Epoch 10/20\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 1.0164 - accuracy: 0.6977\n",
      "Epoch 11/20\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.9538 - accuracy: 0.7239\n",
      "Epoch 12/20\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.8952 - accuracy: 0.7465\n",
      "Epoch 13/20\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.8405 - accuracy: 0.7678 1s - l\n",
      "Epoch 14/20\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7894 - accuracy: 0.7866\n",
      "Epoch 15/20\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.7419 - accuracy: 0.8037\n",
      "Epoch 16/20\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.6979 - accuracy: 0.8194\n",
      "Epoch 17/20\n",
      "63/63 [==============================] - 2s 25ms/step - loss: 0.6571 - accuracy: 0.8332 1s - loss:\n",
      "Epoch 18/20\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.6193 - accuracy: 0.8449\n",
      "Epoch 19/20\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.5844 - accuracy: 0.8555\n",
      "Epoch 20/20\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.5523 - accuracy: 0.8661\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ebd9661af0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.fit(dataset, epochs=20, callbacks=[tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Ty5_Rn5p6Kb"
   },
   "source": [
    "# **HYPER PARAMETER TUNING USING OTHER OPTIMIZERS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "KI-58PDzYhgC"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
    "word2vec.compile(optimizer='Adamax',\n",
    "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "34fKOVQUZMR1",
    "outputId": "c401811e-2e46-48a8-dd09-a6e327bb46bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "63/63 [==============================] - 2s 24ms/step - loss: 1.6091 - accuracy: 0.2143\n",
      "Epoch 2/20\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 1.6063 - accuracy: 0.3045\n",
      "Epoch 3/20\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 1.6030 - accuracy: 0.3863\n",
      "Epoch 4/20\n",
      "63/63 [==============================] - 1s 14ms/step - loss: 1.5982 - accuracy: 0.4546\n",
      "Epoch 5/20\n",
      "63/63 [==============================] - 1s 14ms/step - loss: 1.5916 - accuracy: 0.5001\n",
      "Epoch 6/20\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 1.5829 - accuracy: 0.5250\n",
      "Epoch 7/20\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 1.5722 - accuracy: 0.5351\n",
      "Epoch 8/20\n",
      "63/63 [==============================] - 1s 15ms/step - loss: 1.5599 - accuracy: 0.5352\n",
      "Epoch 9/20\n",
      "63/63 [==============================] - 1s 13ms/step - loss: 1.5464 - accuracy: 0.5321\n",
      "Epoch 10/20\n",
      "63/63 [==============================] - 1s 8ms/step - loss: 1.5319 - accuracy: 0.5281\n",
      "Epoch 11/20\n",
      "63/63 [==============================] - 1s 14ms/step - loss: 1.5169 - accuracy: 0.5261\n",
      "Epoch 12/20\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 1.5015 - accuracy: 0.5242\n",
      "Epoch 13/20\n",
      "63/63 [==============================] - 1s 15ms/step - loss: 1.4859 - accuracy: 0.5247\n",
      "Epoch 14/20\n",
      "63/63 [==============================] - 1s 14ms/step - loss: 1.4702 - accuracy: 0.5254\n",
      "Epoch 15/20\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 1.4545 - accuracy: 0.5277\n",
      "Epoch 16/20\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 1.4389 - accuracy: 0.5299\n",
      "Epoch 17/20\n",
      "63/63 [==============================] - 1s 13ms/step - loss: 1.4233 - accuracy: 0.5328\n",
      "Epoch 18/20\n",
      "63/63 [==============================] - 1s 13ms/step - loss: 1.4077 - accuracy: 0.5358 0s - loss: 1.4073 \n",
      "Epoch 19/20\n",
      "63/63 [==============================] - 1s 14ms/step - loss: 1.3923 - accuracy: 0.5395\n",
      "Epoch 20/20\n",
      "63/63 [==============================] - 1s 13ms/step - loss: 1.3770 - accuracy: 0.5437\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ebe15618b0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
    "word2vec.fit(dataset, epochs=20, callbacks=[tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "0Z79CqOMaCkC"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
    "word2vec.compile(optimizer='Rmsprop',\n",
    "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rhUz1j8baHSw",
    "outputId": "7c9ed710-d25f-4edc-b894-c4b548ec5bdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "63/63 [==============================] - 4s 50ms/step - loss: 1.6080 - accuracy: 0.2371\n",
      "Epoch 2/20\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.5935 - accuracy: 0.4724\n",
      "Epoch 3/20\n",
      "63/63 [==============================] - 1s 14ms/step - loss: 1.5662 - accuracy: 0.5281\n",
      "Epoch 4/20\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 1.5268 - accuracy: 0.5269\n",
      "Epoch 5/20\n",
      "63/63 [==============================] - 1s 14ms/step - loss: 1.4803 - accuracy: 0.5266\n",
      "Epoch 6/20\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.4311 - accuracy: 0.5321 0s - loss: 1.4377 \n",
      "Epoch 7/20\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 1.3818 - accuracy: 0.5428 0s - loss:\n",
      "Epoch 8/20\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 1.3338 - accuracy: 0.5572\n",
      "Epoch 9/20\n",
      "63/63 [==============================] - 1s 14ms/step - loss: 1.2876 - accuracy: 0.5728\n",
      "Epoch 10/20\n",
      "63/63 [==============================] - 1s 14ms/step - loss: 1.2432 - accuracy: 0.5895 0s - loss: 1.2425 - accuracy\n",
      "Epoch 11/20\n",
      "63/63 [==============================] - 1s 14ms/step - loss: 1.2004 - accuracy: 0.6068\n",
      "Epoch 12/20\n",
      "63/63 [==============================] - 1s 14ms/step - loss: 1.1590 - accuracy: 0.6244\n",
      "Epoch 13/20\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.1188 - accuracy: 0.6411\n",
      "Epoch 14/20\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.0798 - accuracy: 0.6579\n",
      "Epoch 15/20\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 1.0419 - accuracy: 0.6753\n",
      "Epoch 16/20\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 1.0049 - accuracy: 0.6913\n",
      "Epoch 17/20\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.9690 - accuracy: 0.7053 0s - loss: 0.9\n",
      "Epoch 18/20\n",
      "63/63 [==============================] - 1s 15ms/step - loss: 0.9340 - accuracy: 0.7190\n",
      "Epoch 19/20\n",
      "63/63 [==============================] - 1s 13ms/step - loss: 0.9000 - accuracy: 0.7329\n",
      "Epoch 20/20\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.8671 - accuracy: 0.7451\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ebe7e38a30>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
    "word2vec.fit(dataset, epochs=20, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "0x6ag2n-LEYw"
   },
   "outputs": [],
   "source": [
    "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JCm_MgxcR6LH",
    "outputId": "30eb9fe2-43fb-4743-87eb-87b040e07395"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uEboRl-nSRsX",
    "outputId": "fa84c707-1f18-4240-aaa2-edc99c0c0726"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "emA_03Y4mo-C",
    "outputId": "ed07cb2b-4b30-4d9a-ba15-60a7eb5c8e17"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5UPFFLEkMVFL",
    "outputId": "bfe329e0-38be-4d7e-b224-430de53fc710"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.2315299 ,  0.07285646, -0.1962219 , -0.0746958 , -0.29530913,\n",
       "        0.09970913,  0.02069301, -0.07089131,  0.03045541, -0.04059046,\n",
       "        0.05053968,  0.35808548, -0.15960693,  0.12220062,  0.20624779,\n",
       "        0.08177194, -0.10249384,  0.289677  ,  0.2643164 ,  0.15399735,\n",
       "       -0.00762504, -0.0556525 ,  0.05075707, -0.321245  ,  0.1341709 ,\n",
       "        0.14681524,  0.04757319, -0.08633887, -0.14138676, -0.00646457,\n",
       "       -0.26118568,  0.11109001,  0.08909423, -0.30923563,  0.08244026,\n",
       "       -0.05547729, -0.1717562 , -0.16588567, -0.08822463, -0.09726227,\n",
       "       -0.05205543, -0.33618903, -0.05267168,  0.2861286 ,  0.18018207,\n",
       "        0.11939853,  0.00803809, -0.22336781, -0.20535085,  0.10816341,\n",
       "        0.08245791,  0.4003676 ,  0.0989694 ,  0.09789625,  0.14066122,\n",
       "        0.17471457,  0.02469319, -0.29544437,  0.05013512, -0.11290441,\n",
       "       -0.19149028, -0.08292069, -0.14902122,  0.2598031 , -0.05147489,\n",
       "        0.20342366, -0.0071078 , -0.21004862,  0.04492521, -0.032865  ,\n",
       "        0.01005568, -0.22522652,  0.19677676,  0.07383887, -0.06907158,\n",
       "        0.14743495, -0.26837096,  0.03320743,  0.12427635,  0.09285425,\n",
       "        0.2628568 ,  0.17274407, -0.15644541,  0.26605293, -0.0997474 ,\n",
       "       -0.36001557, -0.11510115,  0.12505537,  0.19306459, -0.02430606,\n",
       "       -0.19523449, -0.18714857, -0.18122889,  0.09492978, -0.12410712,\n",
       "        0.02067428, -0.20571297, -0.0431882 ,  0.035224  ,  0.14719409],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dV1m-whHNqlY",
    "outputId": "7cf9126f-384a-4630-8d60-f3cc31a856e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[0].shape"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "nlpppppspsps.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
